{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# output metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_metric= {\n",
    "                \"Emissions\":\n",
    "                    [\"emission\", \"co2 emission\", \"ghg emission\", \"emission reduction\", \"emission target\", \"Emissions Policy\"],\n",
    "                \"Water\":\n",
    "                    [\"Water Withdrawal\", \"Water Discharged\", \"Water Recycled\", \"water risk\"],\n",
    "                \"Energy\":\n",
    "                    [\"Energy Consumed\", \"Renewable Energy\"],\n",
    "                \"Business Ethics\":\n",
    "                    [\"Women Executives\", \"Women Board Members\", \"Board Members\", \"Committee Independence\", \"ESG Sustainability Reporting\"],\n",
    "                \"Labor Practices\":\n",
    "                    [\"Trade Union\", \"CEO Salary\", \"Average Salary\", \"Employee Turnover\", \"Avg Training Hours\"],\n",
    "                \"Employee Engagement, Diversity & Inclusion\":\n",
    "                    [\"Women Employees\", \"Women Managers\", \"Minority Employees\"],\n",
    "                \"Employee Health & Safety\":\n",
    "                    [\"Lost Time\", \"Injury Rate\"],\n",
    "                \"Waste\":\n",
    "                    [\"Waste\"]\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import camelot\n",
    "import os\n",
    "\n",
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i : i + n]\n",
    "def get_chunks(filepath, pages, chunk=10):\n",
    "    \"\"\"\n",
    "    Divide the extraction work into n chunks and return this chunks.\n",
    "\n",
    "    filepath : str\n",
    "        Filepath or URL of the PDF file.\n",
    "    pages : str, optional (default: '1')\n",
    "        Comma-separated page numbers.\n",
    "        Example: '1,3,4' or '1,4-end' or 'all'.\n",
    "    \"\"\"\n",
    "\n",
    "    # get list of pages from camelot.handlers.PDFHandler\n",
    "    handler = camelot.handlers.PDFHandler(filepath)\n",
    "    page_list = handler._get_pages(pages=pages)\n",
    "    # chunk pages list\n",
    "    page_chunks = list(chunks(page_list, chunk))\n",
    "\n",
    "    return page_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_information_table(tables):\n",
    "    for table in tables:\n",
    "        # print(table.cols)\n",
    "        # print(table.rows)\n",
    "        # print(tabel.accuracy)\n",
    "        table_df = table.df\n",
    "        columns = table_df.shape[1]\n",
    "        for key_word,value_words  in output_metric.items():\n",
    "            for col in range(columns):\n",
    "                contains = any(table.df[col].str.contains(\"|\".join(value_words), case=False, regex=True).tolist())\n",
    "                if contains:\n",
    "                    position_record_table[key_word].append(str(table.page))\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseTable_camelot(input_path, file, output_path, pages = \"all\"):\n",
    "    print(\"parsing table from \" + file + \" at \"+ pages + \" pages \" )\n",
    "    try:\n",
    "        filepath = os.path.join(input_path, file)\n",
    "        page_chunks = get_chunks(filepath, pages=pages)\n",
    "        for chunk in page_chunks:\n",
    "            pages_string = str(chunk).replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "            tables = camelot.read_pdf(filepath, pages=pages_string)\n",
    "            if len(tables) > 0:\n",
    "                check_information_table(tables)\n",
    "                # print(\"saving {len(tables)} tables\")\n",
    "                # accuracy = [str(table.parsing_report[\"accuracy\"]) for table in tables]\n",
    "                # print(\"accuracy list \" + \" \".join(accuracy))\n",
    "                # tables.export(os.path.join(output_path, file.replace(\"pdf\", \"xlsx\")), f='excel')\n",
    "            else:\n",
    "                print(f\"no tables found for \" + file)\n",
    "    except Exception as e:\n",
    "        print(\"error parsing table from \" + file)    \n",
    "        print(e)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabula\n",
    "import pandas as pd\n",
    "def parseTable_tabula(input_path, file, output_path, pages):\n",
    "    print(\"parsing table from \" + file + \"at pages\" + pages)\n",
    "    try:\n",
    "        tables = tabula.read_pdf(os.path.join(input_path, file), pages=pages, lattice=True)\n",
    "        if len(tables) > 0:\n",
    "            print(\"saving {len(tables)} tables\")\n",
    "            with pd.ExcelWriter(os.path.join(output_path, file.replace(\"pdf\", \"xlsx\"))) as f_obj:\n",
    "                for i,table in enumerate(tables):\n",
    "                    table.to_excel(f_obj, sheet_name=str(i))\n",
    "        else:\n",
    "            print(f\"no tables found for \" + file)\n",
    "    except Exception as e:\n",
    "        print(\"error parsing table from \" + file)    \n",
    "        print(e) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(text):\n",
    "    \"\"\"\n",
    "    tokenize the text\n",
    "    \"\"\"\n",
    "    lower = text.lower()\n",
    "    remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "    no_punctuation = lower.translate(remove_punctuation_map)\n",
    "    tokens = nltk.word_tokenize(no_punctuation)\n",
    "    return tokens\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    \"\"\"\n",
    "    remove morphological affixes from words, leaving only the word stem\n",
    "    \"\"\"\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "def normalize(text):\n",
    "    \"\"\"\n",
    "    applement the tokenize and stem\n",
    "    \"\"\"\n",
    "    tokens = get_tokens(text)\n",
    "    filtered = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    stemmer = SnowballStemmer(\"english\") # PorterStemmer\n",
    "    stemmed = stem_tokens(filtered, stemmer)\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize(\"GHG Emissions Disclosure (cont.) 3.Carbon dioxide equivalent (CO2e) emissions are inclusive of carbon dioxide (CO2),  nitrous oxide (N2O), methane (CH4), and industrial gases such as hydrofluorocarbons  (HFCs), and sulfur hexafluoride (SF6).  Perfluorocarbons (PFCs) and nitrogen trifluoride  (NF3) are not emitted by Tesla’s sites. These carbon dioxide equivalent emissions utilize  Global Warming Potentials (GWPs) defined by the Intergovernmental Panel on Climate  Change’s (IPCC) Fifth Assessment Report (AR5 – 100 year) unless a different  Assessment Report is already embedded in the emission factor source. Carbon dioxide  equivalent emissions are calculated by multiplying actual or estimated energy and fuel  usage by the relevant emission factor taking into account the equivalent GWP. All  emission factors are updated annually where applicable. Management Assertion Scope 1 & 2 GHG Emissions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def find_synonyms(word):\n",
    "    \"\"\"\n",
    "    find the synonyms for given word\n",
    "    \"\"\"\n",
    "    synonyms = []\n",
    "    for syn in wn.synsets(word):\n",
    "        for l in syn.lemmas():\n",
    "            synonyms.append(l.name())\n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(word, count):\n",
    "    return count[word] / sum(count.values()) if sum(count.values()) > 0 else 0\n",
    "def n_containing(word, count_list):\n",
    "    return sum(1 for count in count_list if word in count)\n",
    "def idf(word, count_list):\n",
    "    return math.log(len(count_list)) / (1 + n_containing(word, count_list))\n",
    "def tfidf(word, count, count_list):\n",
    "    return tf(word, count) * idf(word, count_list)\n",
    "\n",
    "def cal_tfidf(text, value_word):\n",
    "    text_stemmed = normalize(text)\n",
    "    count = Counter(text_stemmed)\n",
    "    values = \" \".join(value_word)\n",
    "    values_stemmed = list(set(normalize(values)))\n",
    "    return sum(tf(word, count) * keywords_idf[word]  for word in values_stemmed)\n",
    "    \n",
    "# def cal_tfidf(text_list, key_words=[]):\n",
    "#     countlist = []\n",
    "#     for text in text_list:\n",
    "#         countlist.append(cal_tf(text))\n",
    "#     for i, count in enumerate(countlist):\n",
    "#         print(\"Top words in document {}\".format(i + 1))\n",
    "#         scores = {word: tfidf(word, count, countlist) for word in count}\n",
    "#         sorted_words = sorted(scores.items(), key = lambda x: x[1], reverse=True)\n",
    "#         for word, score in sorted_words[:1]:\n",
    "#             print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "\n",
    "input_path_idf = \"./pdf_files_idf/pdf_files/\"\n",
    "pdf_files = os.listdir(input_path_idf)\n",
    "documents = len(pdf_files)\n",
    "\n",
    "keywords = [\" \".join(value) for key, value in output_metric.items()]\n",
    "keywords = \" \".join(keywords)\n",
    "keywords = set(normalize(keywords))\n",
    "keywords_dict = {key:0 for key in keywords}\n",
    "for file in pdf_files:\n",
    "    try:\n",
    "        word_list = set()\n",
    "        object = PyPDF2.PdfFileReader(os.path.join(input_path_idf, file))\n",
    "        num_pages = object.getNumPages()\n",
    "        for i in range(0, num_pages):\n",
    "            page = object.getPage(i)\n",
    "            text = page.extractText()\n",
    "            text = text.replace('\\n',' ')\n",
    "            text = text.replace(str(i+1), \"\", 1)\n",
    "            text = set(normalize(text))\n",
    "            word_list = word_list.union(text)\n",
    "        for word in keywords_dict.keys():\n",
    "            if word in word_list:\n",
    "                keywords_dict[word] += 1\n",
    "    except:\n",
    "        documents -= 1\n",
    "        print(file + \"is invalid!\")\n",
    "keywords_idf = {key: (math.log(documents) / (1 + value)) for key, value in keywords_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('avg', 1.0729586082894003),\n",
       " ('discharg', 0.45983940355260006),\n",
       " ('co2', 0.24760583268216926),\n",
       " ('withdraw', 0.22991970177630003),\n",
       " ('union', 0.22991970177630003),\n",
       " ('lost', 0.22991970177630003),\n",
       " ('salari', 0.22991970177630003),\n",
       " ('turnov', 0.21459172165788004),\n",
       " ('minor', 0.15327980118420004),\n",
       " ('injuri', 0.15327980118420004),\n",
       " ('esg', 0.1463125374940091),\n",
       " ('trade', 0.1463125374940091),\n",
       " ('ghg', 0.13995112282035654),\n",
       " ('ceo', 0.13995112282035654),\n",
       " ('averag', 0.13411982603617503),\n",
       " ('reduct', 0.13411982603617503),\n",
       " ('hour', 0.13411982603617503),\n",
       " ('consum', 0.13411982603617503),\n",
       " ('women', 0.13411982603617503),\n",
       " ('independ', 0.13411982603617503),\n",
       " ('polici', 0.128755032994728),\n",
       " ('recycl', 0.128755032994728),\n",
       " ('water', 0.128755032994728),\n",
       " ('member', 0.128755032994728),\n",
       " ('renew', 0.128755032994728),\n",
       " ('committe', 0.128755032994728),\n",
       " ('target', 0.128755032994728),\n",
       " ('execut', 0.128755032994728),\n",
       " ('risk', 0.128755032994728),\n",
       " ('train', 0.12380291634108463),\n",
       " ('sustain', 0.12380291634108463),\n",
       " ('time', 0.12380291634108463),\n",
       " ('rate', 0.12380291634108463),\n",
       " ('board', 0.12380291634108463),\n",
       " ('manag', 0.12380291634108463),\n",
       " ('emiss', 0.12380291634108463),\n",
       " ('employe', 0.12380291634108463),\n",
       " ('wast', 0.12380291634108463),\n",
       " ('energi', 0.12380291634108463),\n",
       " ('report', 0.12380291634108463)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(key,value) for key, value in keywords_idf.items()], key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def get_distance(key_word, item):\n",
    "    if w1 in item and w2 in item:\n",
    "        w1_indexes = [index for index, value in enumerate(item) if value == w1]    \n",
    "        w2_indexes = [index for index, value in enumerate(item) if value == w2]    \n",
    "        distances = [abs(item[0] - item[1]) for item in itertools.product(w1_indexes, w2_indexes)]\n",
    "        return {'min': min(distances), 'avg': sum(distances)/float(len(distances))}\n",
    "    else:\n",
    "        return 100\n",
    "def get_proximity(distance):\n",
    "    return 1 / (distance * distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  calculate the relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def score_function(tfidf, tight):\n",
    "    return tfidf * tight\n",
    "def cal_relevance(page, text):\n",
    "    \"\"\"\n",
    "    check whether the text contains the key words\n",
    "    step1: regular expressions\n",
    "    step2: cal relevance\n",
    "    \"\"\"\n",
    "    for key_word, value_words in output_metric.items():\n",
    "        # pattern_words = []\n",
    "        # for word in value_words:\n",
    "        #     pattern_words.extend(normalize(word))\n",
    "        # pattern_words = list(set(pattern_words))\n",
    "        # pattern = \"|\".join(value_words)\n",
    "        # rex = re.search(pattern, text, re.I)\n",
    "        # if rex != None:\n",
    "        position_record_text[key_word].append((str(page),score_function(cal_tfidf(text, value_words), )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "def parseText(input_path, file, output_path, pages):\n",
    "    print(\"parsing text from \" + file + \" at \" + pages + \" pages \")\n",
    "    object = PyPDF2.PdfFileReader(os.path.join(input_path, file))\n",
    "    num_pages = object.getNumPages()\n",
    "    for i in range(0, num_pages):\n",
    "        page = object.getPage(i)\n",
    "        text = page.extractText()\n",
    "        text = text.replace('\\n',' ')\n",
    "        text = text.replace(str(i+1), \"\", 1) \n",
    "        cal_relevance(i, text)\n",
    "        # with open(os.path.join(output_path, file.replace(\"pdf\", \"text\")), \"a\") as f_obj:\n",
    "        #     f_obj.write(text+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./text_files/2021-tesla-impact-report.text\", \"r\") as f_obj:\n",
    "#     text_list = f_obj.readlines()\n",
    "# cal_tfidf(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing text from 2021-tesla-impact-report.pdf at all pages \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "input_path = \"./pdf_files\"\n",
    "output_table_camelot = \"./table_files_camelot\"\n",
    "output_table_tabula = \"./table_files_tabula\"\n",
    "output_path_text = \"./text_files\"\n",
    "pdf_files = os.listdir(input_path)\n",
    "name = [\"output_metric\", \"page\"]\n",
    "for file in pdf_files:\n",
    "    # 1 parsing the table and text\n",
    "    position_record_table = {key_word:[] for key_word in output_metric.keys()}\n",
    "    position_record_text = {key_word:[] for key_word in output_metric.keys()}\n",
    "    # parseTable_camelot(input_path, file, output_table_camelot, pages=\"all\")\n",
    "    # parseTable_tabula(input_path, file, output_table_tabula)\n",
    "    parseText(input_path, file, output_path_text, \"all\")\n",
    "    # 2 choose the top 3 pages based on the term frequency\n",
    "    for keys in position_record_text.keys():\n",
    "        position_record_text[keys] = [x[0] for x in sorted(position_record_text[keys], \n",
    "                                      key= lambda x: x[1], reverse=True)][:3]\n",
    "    # 3 save the result\n",
    "    position_record = {key_word:position_record_table[key_word]+position_record_text[key_word] \n",
    "                       for key_word in output_metric.keys()}\n",
    "    position_record = {key_word:list(set(pages)) for key_word, pages in position_record.items()}\n",
    "    position_matchs = [[key, \" \".join(value)] for key, value in position_record.items()]\n",
    "    data = pd.DataFrame(columns=name, data=position_matchs)\n",
    "    data.to_csv(\"./data_extraction/\" + file.replace(\".pdf\", \"_text_tfidf.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('camelotenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0223c632f617fca949c57c3ca0f5d1cb8bdc1aa423ed0406ba10b84d4d8863b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
